{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "793e90f9",
   "metadata": {},
   "source": [
    "# Logistic Regression on Canada Per Capita Income (from scratch)\n",
    "\n",
    "Uses only NumPy for math and Matplotlib for visualization; implements binary logistic regression with gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012505e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bee285",
   "metadata": {},
   "source": [
    "## 1. Load the CSV dataset\n",
    "Dataset: `canada_per_capita_income.csv` (year, income). We create a binary label: income above the median = 1, else 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6da3bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(csv_path):\n",
    "    \"\"\"Load year/income CSV with NumPy only.\"\"\"\n",
    "    data = np.loadtxt(csv_path, delimiter=\",\", skiprows=1)\n",
    "    x = data[:, 0:1]  # year\n",
    "    y_cont = data[:, 1:1+1]  # income\n",
    "    return x, y_cont\n",
    "\n",
    "# Load data\n",
    "csv_path = \"canada_per_capita_income.csv\"\n",
    "x_raw, y_cont = load_dataset(csv_path)\n",
    "print(f\"Loaded {x_raw.shape[0]} rows from {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2233f29",
   "metadata": {},
   "source": [
    "## 2. Prepare data and labels\n",
    "We binarize income around the median: label 1 if income ≥ median, else 0. Split train/test (80/20)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcf907d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(x, y, test_ratio=0.2):\n",
    "    \"\"\"Manual shuffle split.\"\"\"\n",
    "    m = x.shape[0]\n",
    "    idx = np.random.permutation(m)\n",
    "    test_size = int(m * test_ratio)\n",
    "    test_idx = idx[:test_size]\n",
    "    train_idx = idx[test_size:]\n",
    "    return x[train_idx], x[test_idx], y[train_idx], y[test_idx]\n",
    "\n",
    "# Create binary labels and split\n",
    "median_income = np.median(y_cont)\n",
    "y = (y_cont >= median_income).astype(int)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_raw, y, test_ratio=0.2)\n",
    "print(f\"Train size: {x_train.shape[0]}, Test size: {x_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7153fc",
   "metadata": {},
   "source": [
    "## 3. Model equations (univariate logistic regression)\n",
    "- Sigmoid: $\\sigma(z) = \\dfrac{1}{1 + e^{-z}}$\n",
    "- Hypothesis: $\\hat{y} = \\sigma(w_0 + w_1 x)$\n",
    "- Cost (binary cross-entropy): $J(w) = -\\dfrac{1}{m}\\sum_{i=1}^m\\big[y^{(i)}\\log \\hat{y}^{(i)} + (1-y^{(i)})\\log(1-\\hat{y}^{(i)})\\big]$\n",
    "- Gradients:\n",
    "  - $\\dfrac{\\partial J}{\\partial w_0} = \\dfrac{1}{m}\\sum (\\hat{y} - y)$\n",
    "  - $\\dfrac{\\partial J}{\\partial w_1} = \\dfrac{1}{m}\\sum (\\hat{y} - y)x$\n",
    "- Updates (gradient descent): $w \\leftarrow w - \\alpha \\nabla J$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c7d28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_feature(x):\n",
    "    mean = np.mean(x)\n",
    "    std = np.std(x) + 1e-8\n",
    "    return (x - mean) / std, mean, std\n",
    "\n",
    "\n",
    "def add_bias(x):\n",
    "    return np.hstack([np.ones((x.shape[0], 1)), x])\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-np.clip(z, -500, 500)))\n",
    "\n",
    "\n",
    "def predict_proba(X, w):\n",
    "    return sigmoid(X @ w)\n",
    "\n",
    "\n",
    "def compute_cost(X, y, w):\n",
    "    m = X.shape[0]\n",
    "    p = predict_proba(X, w)\n",
    "    eps = 1e-15\n",
    "    p = np.clip(p, eps, 1 - eps)\n",
    "    return -np.mean(y * np.log(p) + (1 - y) * np.log(1 - p))\n",
    "\n",
    "\n",
    "def compute_gradient(X, y, w):\n",
    "    m = X.shape[0]\n",
    "    p = predict_proba(X, w)\n",
    "    return (1.0 / m) * (X.T @ (p - y))\n",
    "\n",
    "\n",
    "def gradient_descent(X, y, alpha=0.1, num_iters=2000, verbose=False):\n",
    "    w = np.zeros((X.shape[1], 1))\n",
    "    costs = []\n",
    "    for i in range(num_iters):\n",
    "        grad = compute_gradient(X, y, w)\n",
    "        w -= alpha * grad\n",
    "        if verbose and i % 200 == 0:\n",
    "            cost = compute_cost(X, y, w)\n",
    "            costs.append(cost)\n",
    "            print(f\"Iter {i:4d} | Cost {cost:.6f}\")\n",
    "        elif not verbose:\n",
    "            costs.append(compute_cost(X, y, w))\n",
    "    return w, costs\n",
    "\n",
    "\n",
    "def predict(X, w, threshold=0.5):\n",
    "    return (predict_proba(X, w) >= threshold).astype(int)\n",
    "\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a33467",
   "metadata": {},
   "source": [
    "## 4. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa85096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize feature, build design matrix, and run gradient descent\n",
    "x_train_norm, x_mean, x_std = normalize_feature(x_train)\n",
    "x_test_norm = (x_test - x_mean) / x_std\n",
    "\n",
    "X_train = add_bias(x_train_norm)\n",
    "X_test = add_bias(x_test_norm)\n",
    "\n",
    "alpha = 0.3\n",
    "num_iters = 1500\n",
    "\n",
    "w, costs = gradient_descent(X_train, y_train, alpha=alpha, num_iters=num_iters, verbose=True)\n",
    "\n",
    "print(\"Training complete.\")\n",
    "print(f\"Final cost: {costs[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2f673f",
   "metadata": {},
   "source": [
    "## 5. Evaluate and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19af6f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions and accuracy\n",
    "y_train_pred = predict(X_train, w)\n",
    "y_test_pred = predict(X_test, w)\n",
    "\n",
    "train_acc = accuracy(y_train, y_train_pred)\n",
    "test_acc = accuracy(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Train accuracy: {train_acc:.2f}%\")\n",
    "print(f\"Test accuracy:  {test_acc:.2f}%\")\n",
    "\n",
    "# Plot data and decision boundary\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(x_train, y_train, label=\"Train\", color=\"steelblue\", alpha=0.7)\n",
    "plt.scatter(x_test, y_test, label=\"Test\", color=\"orange\", alpha=0.9, marker=\"x\")\n",
    "\n",
    "x_line = np.linspace(x_raw.min(), x_raw.max(), 200).reshape(-1, 1)\n",
    "x_line_norm = (x_line - x_mean) / x_std\n",
    "X_line = add_bias(x_line_norm)\n",
    "probs = predict_proba(X_line, w)\n",
    "\n",
    "plt.plot(x_line, probs, color=\"crimson\", linewidth=2.5, label=\"Predicted P(y=1|x)\")\n",
    "plt.axhline(0.5, color=\"gray\", linestyle=\"--\", linewidth=1)\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"P(income ≥ median)\")\n",
    "plt.title(\"Logistic Regression Decision Curve\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot cost curve\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(costs, color=\"darkgreen\", linewidth=2)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Cost (Binary Cross-Entropy)\")\n",
    "plt.title(\"Cost vs. iterations\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
