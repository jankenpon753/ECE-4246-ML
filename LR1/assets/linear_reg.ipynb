{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5a54bb4",
   "metadata": {},
   "source": [
    "# Linear Regression on Canada Per Capita Income (from scratch)\n",
    "\n",
    "Implementation uses only NumPy for math and Matplotlib for visualization, following the standard gradient descent solution for univariate linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aadbd3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Plot style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f728bb11",
   "metadata": {},
   "source": [
    "## 1. Load the CSV dataset\n",
    "Dataset: `canada_per_capita_income.csv` (year, income)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b6bd8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 47 rows from canada_per_capita_income.csv\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(csv_path):\n",
    "    \"\"\"\n",
    "    Load a two-column CSV (year, income) without using pandas.\n",
    "    Returns feature column x (years) and target y (income), both as column vectors.\n",
    "    \"\"\"\n",
    "    data = np.loadtxt(csv_path, delimiter=\",\", skiprows=1)\n",
    "    x = data[:, 0:1]  # years\n",
    "    y = data[:, 1:1+1]  # income\n",
    "    return x, y\n",
    "\n",
    "csv_path = \"canada_per_capita_income.csv\"\n",
    "x_raw, y = load_dataset(csv_path)\n",
    "print(f\"Loaded {x_raw.shape[0]} rows from {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425235d9",
   "metadata": {},
   "source": [
    "## 2. Model equations (univariate linear regression)\n",
    "- Hypothesis: $\\hat{y} = w_0 + w_1 x$\n",
    "- Cost (MSE): $J(w_0, w_1) = \\dfrac{1}{2m} \\sum_{i=1}^{m} (\\hat{y}^{(i)} - y^{(i)})^2$\n",
    "- Gradients:\n",
    "  - $\\dfrac{\\partial J}{\\partial w_0} = \\dfrac{1}{m} \\sum_{i=1}^{m} (\\hat{y}^{(i)} - y^{(i)})$\n",
    "  - $\\dfrac{\\partial J}{\\partial w_1} = \\dfrac{1}{m} \\sum_{i=1}^{m} (\\hat{y}^{(i)} - y^{(i)}) x^{(i)}$\n",
    "- Updates (gradient descent):\n",
    "  - $w_0 \\leftarrow w_0 - \\alpha \\dfrac{\\partial J}{\\partial w_0}$\n",
    "  - $w_1 \\leftarrow w_1 - \\alpha \\dfrac{\\partial J}{\\partial w_1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "521d914a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_feature(x):\n",
    "    \"\"\"\n",
    "    Normalize feature to zero mean and unit variance for stable gradient descent.\n",
    "    Returns normalized x and the (mean, std) for inverse transforms.\n",
    "    \"\"\"\n",
    "    mean = np.mean(x)\n",
    "    std = np.std(x) + 1e-8\n",
    "    x_norm = (x - mean) / std\n",
    "    return x_norm, mean, std\n",
    "\n",
    "\n",
    "def add_bias(x):\n",
    "    \"\"\"\n",
    "    Add bias column of ones to feature matrix.\n",
    "    \"\"\"\n",
    "    return np.hstack([np.ones((x.shape[0], 1)), x])\n",
    "\n",
    "\n",
    "def predict(X, w):\n",
    "    \"\"\"Compute predictions given design matrix X and weights w.\"\"\"\n",
    "    return X @ w\n",
    "\n",
    "\n",
    "def compute_cost(X, y, w):\n",
    "    \"\"\"Mean squared error cost: J = (1/(2m)) * sum((Xw - y)^2).\"\"\"\n",
    "    m = X.shape[0]\n",
    "    residual = predict(X, w) - y\n",
    "    return (1.0 / (2 * m)) * np.sum(residual ** 2)\n",
    "\n",
    "\n",
    "def compute_gradient(X, y, w):\n",
    "    \"\"\"Gradient of MSE: (1/m) * X^T (Xw - y).\"\"\"\n",
    "    m = X.shape[0]\n",
    "    residual = predict(X, w) - y\n",
    "    return (1.0 / m) * (X.T @ residual)\n",
    "\n",
    "\n",
    "def gradient_descent(X, y, alpha=0.01, num_iters=1000, verbose=False):\n",
    "    \"\"\"\n",
    "    Batch gradient descent for linear regression.\n",
    "    Returns learned weights and cost history.\n",
    "    \"\"\"\n",
    "    w = np.zeros((X.shape[1], 1))\n",
    "    costs = []\n",
    "    for i in range(num_iters):\n",
    "        grad = compute_gradient(X, y, w)\n",
    "        w -= alpha * grad\n",
    "        if verbose and i % 100 == 0:\n",
    "            cost = compute_cost(X, y, w)\n",
    "            costs.append(cost)\n",
    "            print(f\"Iter {i:4d} | Cost {cost:.6f}\")\n",
    "        elif not verbose:\n",
    "            costs.append(compute_cost(X, y, w))\n",
    "    return w, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9af41a",
   "metadata": {},
   "source": [
    "## 3. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaefe376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize feature, build design matrix, and run gradient descent\n",
    "x_norm, x_mean, x_std = normalize_feature(x_raw)\n",
    "X = add_bias(x_norm)\n",
    "\n",
    "alpha = 0.1\n",
    "num_iters = 1500\n",
    "\n",
    "w, costs = gradient_descent(X, y, alpha=alpha, num_iters=num_iters, verbose=True)\n",
    "\n",
    "print(\"Training complete.\")\n",
    "print(f\"Final cost: {costs[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa1e552",
   "metadata": {},
   "source": [
    "## 4. Evaluate and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf5c137",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize_weights(w, mean, std):\n",
    "    \"\"\"\n",
    "    Convert weights learned on normalized x back to original scale.\n",
    "    Returns slope and intercept in original units.\n",
    "    \"\"\"\n",
    "    w0, w1 = w.flatten()\n",
    "    slope = w1 / std\n",
    "    intercept = w0 - (w1 * mean / std)\n",
    "    return intercept, slope\n",
    "\n",
    "\n",
    "# Recover slope/intercept in original scale\n",
    "intercept, slope = denormalize_weights(w, x_mean, x_std)\n",
    "print(f\"Model: income = {slope:.4f} * year + {intercept:.4f}\")\n",
    "\n",
    "# Predictions for training points\n",
    "pred_train = predict(X, w)\n",
    "\n",
    "# Metrics\n",
    "m = y.shape[0]\n",
    "rmse = np.sqrt((1.0 / m) * np.sum((pred_train - y) ** 2))\n",
    "mae = (1.0 / m) * np.sum(np.abs(pred_train - y))\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAE:  {mae:.4f}\")\n",
    "\n",
    "# Plot data and regression line\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(x_raw, y, color=\"steelblue\", label=\"Data\")\n",
    "\n",
    "# Line over feature range\n",
    "x_line = np.linspace(x_raw.min(), x_raw.max(), 100).reshape(-1, 1)\n",
    "x_line_norm = (x_line - x_mean) / x_std\n",
    "X_line = add_bias(x_line_norm)\n",
    "y_line = predict(X_line, w)\n",
    "\n",
    "plt.plot(x_line, y_line, color=\"crimson\", linewidth=2.5, label=\"Fitted line\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Per Capita Income\")\n",
    "plt.title(\"Linear Regression Fit\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot cost over iterations\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(costs, color=\"darkgreen\", linewidth=2)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Cost (MSE * 0.5)\")\n",
    "plt.title(\"Cost vs. iterations\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebookEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
